{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b45df88-47c0-436e-96e5-4b74c3cfe073",
   "metadata": {},
   "source": [
    "# Dependency locality and information locality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179902d7-221d-4c22-936a-d68db4e884e2",
   "metadata": {},
   "source": [
    "This exercise aims to reproduce and evaluate the methodology related to assess dependency locality and information locality in natural languages. \n",
    "\n",
    "We expect you to work in binoms and return a document commenting your answers. This document will be supported by the code in the notebook.\n",
    "\n",
    "The goal is to figure out how to carry out experiments like those described in the  paper *Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal* by Hahn, Degen and Futrell.\n",
    "\n",
    "We will investigate how to design language models with limited memory and how to use them to answer questions on natural and manipulated datasets. The first part of the exercise is methodological. It amounts to design and set up non standard language models. The second part of the exercise aims to investigate how to use these models to answer scientific questions.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b07599-0a14-4995-ab33-9310e21bbc96",
   "metadata": {},
   "source": [
    "**The answers to the exercises have to be returned as a written document (approx 5 pages). The code will be given in appendix. The grading uses mainly the text.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569b50f-48f7-48bb-a97f-6936125463a3",
   "metadata": {},
   "source": [
    "## Memory limited neural language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489d384-e46d-42fc-a821-1ef51dc75cf0",
   "metadata": {},
   "source": [
    "We consider two types of language models, first Neural Network Language models (NNLM) then Recurrent Neural Network Language Models (RNNLM)\n",
    "These are models predicting the $i-$th word given the $k$ previous ones. We provide an example implementation for an NNLM and you will have to design an implementation for the RNNLM in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3c120-3da7-4b67-ba0f-0b1cacd699ee",
   "metadata": {},
   "source": [
    "### The tokenizer\n",
    "\n",
    "The first step is to implement a tokenizer. The tokenizer plays two roles\n",
    " - it splits the input string into tokens\n",
    " - it maps the tokens to integer codes\n",
    "\n",
    "We design a class that does exactly that. Note that we have to add some artificial tokens to handle unknown words for instance\n",
    "\n",
    "The example tokenizer follows loosely the [HuggingFace tokenizer](https://huggingface.co/docs/transformers/en/main_classes/tokenizer) interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b196a45f-ded5-43ae-bdc0-451985e0bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import NaiveTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9a87a0-9a84-438b-ad2b-da2e74303035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codes [2, 3, 4, 0, 0, 5, 6]\n",
      "toks ['Language', 'models', 'are', 'not', 'so', 'cool', '.']\n",
      "\n",
      "padded codes\n",
      "tensor([[2, 3, 4, 5, 6, 1, 1],\n",
      "        [2, 3, 4, 0, 0, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Toyish example\n",
    "# Create the tokenizer with some known vocabulary\n",
    "\n",
    "# observe how the unknown vocabulary is encoded\n",
    "# observe how padding is encoded too\n",
    "\n",
    "tokenizer = NaiveTokenizer(\"Language models are cool .\".split())\n",
    "codes     = tokenizer(\"Language models are not so cool .\")\n",
    "toks      = tokenizer.tokenize(\"Language models are not so cool .\")\n",
    "\n",
    "print(\"codes\",codes)\n",
    "print(\"toks\",toks)\n",
    "\n",
    "#Example of padding\n",
    "print(\"\\npadded codes\")\n",
    "batch = [tokenizer(sentence) for sentence in [\"Language models are cool .\",\"Language models are not so cool .\"]]\n",
    "print(tokenizer.pad_batch(batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d7766-d2fb-4cb7-9289-956c878672d8",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader\n",
    "\n",
    "[Dataset and Dataloader](https://pytorch.org/docs/stable/data.html) are pytorch classes that are used to load efficiently a dataset by batches on one or several GPUs. Here we provide a naive introductory example for language modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd311bfe-5607-4448-bbe3-969b1c3ab8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader output\n",
      "\n",
      "tensor([[30, 31, 15, 32,  6],\n",
      "        [33,  3, 34, 10, 11],\n",
      "        [25, 11, 26, 27, 11],\n",
      "        [ 7, 52, 18, 57, 11],\n",
      "        [34, 10, 11, 35, 13],\n",
      "        [11, 40, 45, 11, 46],\n",
      "        [ 9, 10, 11, 13, 44],\n",
      "        [25, 11, 13, 47, 11],\n",
      "        [ 3, 34, 10, 11, 13],\n",
      "        [10, 11, 35, 13,  6]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[ 7, 20, 13, 18, 24],\n",
      "        [40, 45, 11, 46,  6],\n",
      "        [40, 41, 42, 43,  9],\n",
      "        [51, 31, 22, 52, 53],\n",
      "        [27, 11, 28, 13,  6],\n",
      "        [18, 57, 11, 58,  6],\n",
      "        [43,  9, 10, 11, 13],\n",
      "        [25, 11, 40, 45, 11],\n",
      "        [18, 19, 10, 11, 20],\n",
      "        [13, 44, 25, 11, 40]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[20, 13, 18, 24, 25],\n",
      "        [10, 11, 37, 13,  6],\n",
      "        [44, 25, 11, 40, 45],\n",
      "        [47, 11, 48, 18, 49],\n",
      "        [13, 18, 24, 25, 11],\n",
      "        [24, 25, 11, 26, 27],\n",
      "        [19, 10, 11, 20, 13],\n",
      "        [29, 30, 31, 15, 32],\n",
      "        [ 2,  3,  4,  5,  6],\n",
      "        [14, 15, 11, 16,  6]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[ 3, 34, 10, 11, 35],\n",
      "        [10, 11, 20, 13,  6],\n",
      "        [ 7, 21, 22, 23,  6],\n",
      "        [10, 11, 13, 44, 25],\n",
      "        [11, 13, 44, 25, 11],\n",
      "        [18, 19, 10, 11, 37],\n",
      "        [13, 44, 25, 11, 13],\n",
      "        [13, 47, 11, 48, 18],\n",
      "        [38,  9, 44, 25, 11],\n",
      "        [17, 18, 19, 10, 11]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[50, 51, 31, 22, 52],\n",
      "        [11, 48, 18, 49,  6],\n",
      "        [ 9, 44, 25, 11, 56],\n",
      "        [26, 27, 11, 28, 13],\n",
      "        [ 7, 14, 15, 11, 16],\n",
      "        [44, 25, 11, 13, 47],\n",
      "        [31, 22, 52, 53,  6],\n",
      "        [41, 42, 43,  9, 10],\n",
      "        [ 7, 38,  9, 10, 11],\n",
      "        [11, 26, 27, 11, 28]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[42, 43,  9, 10, 11],\n",
      "        [52, 18, 57, 11, 58],\n",
      "        [ 7, 38,  9, 44, 25],\n",
      "        [34, 10, 11, 13, 44],\n",
      "        [10, 11, 39, 13,  6],\n",
      "        [33,  3, 34, 10, 11],\n",
      "        [ 7, 40, 41, 42, 43],\n",
      "        [ 7, 54, 42, 55,  6],\n",
      "        [38,  9, 10, 11, 39],\n",
      "        [19, 10, 11, 37, 13]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[10, 11, 13, 44, 25],\n",
      "        [44, 25, 11, 56, 13],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [ 9, 10, 11, 12, 13],\n",
      "        [ 9, 10, 11, 39, 13],\n",
      "        [10, 11, 12, 13,  6],\n",
      "        [11, 13, 44, 25, 11],\n",
      "        [ 8,  9, 10, 11, 12],\n",
      "        [25, 11, 56, 13,  6],\n",
      "        [18, 24, 25, 11, 26]])\n",
      "batch size: torch.Size([10, 5])\n",
      "tensor([[ 7, 50, 51, 31, 22],\n",
      "        [36, 18, 19, 10, 11],\n",
      "        [ 7, 29, 30, 31, 15],\n",
      "        [11, 13, 47, 11, 48]])\n",
      "batch size: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "from utils.dataLoader import NgramsLanguageModelDataSet\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.sentenceProcessing import normalize\n",
    "from models.MarkovianNN import NNLM \n",
    "\n",
    "\n",
    "#Dummy example to see what it does\n",
    "\n",
    "zebra_dataset = \"\"\"\n",
    "There are five houses.\n",
    "The Englishman lives in the red house.\n",
    "The Spaniard owns the dog.\n",
    "Coffee is drunk in the green house.\n",
    "The Ukrainian drinks tea.\n",
    "The green house is immediately to the right of the ivory house.\n",
    "The Old Gold smoker owns snails.\n",
    "Kools are smoked in the yellow house.\n",
    "Milk is drunk in the middle house.\n",
    "The Norwegian lives in the first house.\n",
    "The man who smokes Chesterfields lives in the house next to the man with the fox.\n",
    "Kools are smoked in the house next to the house where the horse is kept.\n",
    "The Lucky Strike smoker drinks orange juice.\n",
    "The Japanese smokes Parliaments.\n",
    "The Norwegian lives next to the blue house.\n",
    "\"\"\"\n",
    "sentences = [normalize(sent) for sent in zebra_dataset.split('\\n') if sent and not sent.isspace()]\n",
    "tokenizer = NaiveTokenizer(normalize(zebra_dataset).split())\n",
    "dataset   = NgramsLanguageModelDataSet(5,sentences,tokenizer) #pentagram dataset\n",
    "\n",
    "print('DataLoader output\\n')\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True,collate_fn=tokenizer.pad_batch)\n",
    "for batch in dataloader:\n",
    "  print(batch)\n",
    "  size = batch.size()\n",
    "  print(\"batch size:\",size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6975769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931ab54-5257-4114-ad59-34dcca47153f",
   "metadata": {},
   "source": [
    "import torch\n",
    "from models.MarkovianNN import NNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22765c6-10dd-46bc-9154-9f39378caefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1,  1,  7],\n",
      "        [ 1,  1,  1,  7, 50],\n",
      "        [ 1,  1,  7, 50, 51],\n",
      "        [ 1,  7, 50, 51, 31],\n",
      "        [ 7, 50, 51, 31, 22],\n",
      "        [50, 51, 31, 22, 52],\n",
      "        [51, 31, 22, 52,  0],\n",
      "        [31, 22, 52,  0,  6]])\n",
      "The Lucky Strike smoker drinks orange <unk> .\n",
      "[('The', -4.306552886962891), ('Lucky', -4.249198913574219), ('Strike', -4.165590286254883), ('smoker', -4.255760192871094), ('drinks', -4.107135772705078), ('orange', -4.00046968460083), ('<unk>', -4.232947826385498), ('.', -3.3983154296875)]\n",
      "tensor([[ 1,  1,  1,  1,  7],\n",
      "        [ 1,  1,  1,  7, 50],\n",
      "        [ 1,  1,  7, 50, 51],\n",
      "        [ 1,  7, 50, 51, 31],\n",
      "        [ 7, 50, 51, 31,  0],\n",
      "        [50, 51, 31,  0, 52],\n",
      "        [51, 31,  0, 52,  6]])\n",
      "The Lucky Strike smoker <unk> orange .\n",
      "[('The', -4.306552886962891), ('Lucky', -4.249198913574219), ('Strike', -4.165590286254883), ('smoker', -4.255760192871094), ('<unk>', -4.232947826385498), ('orange', -4.00046968460083), ('.', -3.3983154296875)]\n",
      "tensor([[ 1,  1,  1,  1,  7],\n",
      "        [ 1,  1,  1,  7, 50],\n",
      "        [ 1,  1,  7, 50, 51],\n",
      "        [ 1,  7, 50, 51, 31],\n",
      "        [ 7, 50, 51, 31, 22],\n",
      "        [50, 51, 31, 22,  0],\n",
      "        [51, 31, 22,  0,  6]])\n",
      "The Lucky Strike smoker drinks <unk> .\n",
      "[('The', -4.306552886962891), ('Lucky', -4.249198913574219), ('Strike', -4.165590286254883), ('smoker', -4.255760192871094), ('drinks', -4.107135772705078), ('<unk>', -4.232947826385498), ('.', -3.3983154296875)]\n"
     ]
    }
   ],
   "source": [
    "lang_model = NNLM(128,tokenizer.vocab_size,128,5-1,tokenizer.pad_id)\n",
    "lang_model.train(dataloader,40,device='cpu') #or 'cuda' to run on a GPU or 'mps' for latest macos\n",
    "\n",
    "#predicting with the model\n",
    "\n",
    "test_set    = NgramsLanguageModelDataSet(5,[\"The Lucky Strike smoker drinks orange milk .\"],tokenizer, left_padding=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False,collate_fn=tokenizer.pad_batch)\n",
    "\n",
    "for batch in test_loader:\n",
    "    predictions = lang_model(batch)\n",
    "    print(batch)\n",
    "    src    = tokenizer.decode_ngram(batch)\n",
    "    print(src)\n",
    "    logits = lang_model(batch)\n",
    "    print(list(zip(src.split(),logits.tolist())))\n",
    "   \n",
    "    \n",
    "test_set    = NgramsLanguageModelDataSet(5,[\"The Lucky Strike smoker eat orange .\"],tokenizer, left_padding=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False,collate_fn=tokenizer.pad_batch)\n",
    "\n",
    "for batch in test_loader:\n",
    "    predictions = lang_model(batch)\n",
    "    print(batch)\n",
    "    src    = tokenizer.decode_ngram(batch)\n",
    "    print(src)\n",
    "    logits = lang_model(batch)\n",
    "    print(list(zip(src.split(),logits.tolist())))\n",
    "\n",
    "\n",
    "test_set    = NgramsLanguageModelDataSet(5,[\"The Lucky Strike smoker drinks bear .\"],tokenizer, left_padding=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False,collate_fn=tokenizer.pad_batch)\n",
    "\n",
    "for batch in test_loader:\n",
    "    predictions = lang_model(batch)\n",
    "    print(batch)\n",
    "    src    = tokenizer.decode_ngram(batch)\n",
    "    print(src)\n",
    "    logits = lang_model(batch)\n",
    "    print(list(zip(src.split(),logits.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff56f3b-ce98-4b7a-af6b-7437f2536451",
   "metadata": {},
   "source": [
    "**Exercise 1 (1pt)** Read the code above and try to understand it.  Why do we see only the last few tokens of the test sentence ? This is a common issue in many language modeling implementations. Provide a correction to the code and explain what was the cause. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6eaf2-eb22-4ea4-bae9-1a0127162169",
   "metadata": {},
   "source": [
    "**Exercise 2 (1pt)** Try to understand how the language model  handles unknown words. Is it the same method as GPT-2 ? explain the differences.\n",
    "Can we easily compare the suprisals returned by the two models ? what is your opinion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f70c88",
   "metadata": {},
   "source": [
    "GPT-2 uses **byte-level BPE tokenizer**, where any string can be expressed as a sequence of subword merges down to raw-8 bytes, whereas our model only treat unknown words as a single tag *<unk>*, and their suprisals are the same. Hence, we cannot compare the surprisals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769be52-5adf-4311-9a51-ecb1541efddd",
   "metadata": {},
   "source": [
    "**Exercise 3 (3pts)** Given the code above, implement a Recurrent Neural Network Language Model trainable on ngrams. The dataset and the dataloader can be reused as is. The main difference is located in the Module subclass. This time we do not concatenate the embeddings of the past tokens but rather the model uses an RNN module. Pytorch library provides an LSTM module and there are [several tutorials explaining how to implement those models online](https://docs.pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html).  \n",
    "\n",
    "Explain what are the main challenges related to implementing LSTM like models in pytorch. Do you manage to run your models on cuda devices ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f522c-1b70-402a-bf20-753bc464d3a3",
   "metadata": {},
   "source": [
    "## Generating artificial datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af304d-6954-4259-86df-0fc66f5bb52b",
   "metadata": {},
   "source": [
    "In order to test to which extent human language minimize dependency length and enforce information locality,\n",
    "we will compare word order in natural corpus with word order in corpora where word order is somewhat randomized.\n",
    "The corpora used here are taken from Universal Dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d9f8f-cd04-47f3-b831-5015e9a31a1f",
   "metadata": {},
   "source": [
    "**Exercise 4 (1pt)** Download the UD corpora from the website and choose 10 languages with which you will work. Some of them should be free word order some \n",
    "of them should be fixed word order. Ideally corpora should be large enough to estimate language models. To figure out whether a language is free or fixed word order you may look at resources like the [World Atlas of Language Structures](https://wals.info)\n",
    "Write an explanation on how you chose your corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71863e-de62-4f89-8eb8-0d62d50ed0a9",
   "metadata": {},
   "source": [
    "**Exercise 5 (4pt)** Generate artificial corpora from the natural corpora. Those artificial corpora will have randomized word order. \n",
    "The `dependency.py` module given in the git may help you read the corpora and shuffle the dependency trees ? \n",
    "As controlling randomness is a key issue you may wish to consult the paper *Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal* \n",
    "and decide if the default randomization procedures suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52ea6e-a7bd-4dce-88dc-158923e0783a",
   "metadata": {},
   "source": [
    "## Dependency Length experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ee12a-54ff-4e74-b4aa-5a6b3c0e5168",
   "metadata": {},
   "source": [
    "**Exercise 6 (5pts)** Try to answer the following questions. Is there a significative difference between dependency length observed in natural corpora with the dependency length observed in corpora with a randomized word order ? Illustrate the differences with relevant plots. Can you identify an hypothesis test that would allow to test the observed differences ? Do languages said to be free word order have indeed longer average dependency lengths ?  How important is the choice of the randomization procedure to reach your conclusion ? You may combine observations from plots and also relevant hypothesis testing to support your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919533c3-ad92-4c25-8239-c5fbe0ca924f",
   "metadata": {},
   "source": [
    "## Information Locality experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac1a0e-4ec3-401a-9fa4-b2eb7974ee5c",
   "metadata": {},
   "source": [
    "**Exercise 7(4 pts)** Mirroring the previous exercise, we aim to assess for locality effects in natural language without measuring dependency lengths but rather using information theoretic measures computed from language models. To test the relevance of the information locality theorem, train language models with different memory sizes (NNLM, RNNLM or others) and for each of your corpora, compare the average surprisals of the models as a function of the memory size. What do you observe ? how do you interpret the results ? How would you compare the results across languages with different word order properties? is it straightforward ? explain the problematics and illustrate your conclusions with plots and/or hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a93dd-62aa-4ab1-bd86-751077e6003b",
   "metadata": {},
   "source": [
    "**Exercise 8 (1pt)** It would be tempting to use a multilingual large language model here. Is it possible ? conjecture whether they could be used or not ? what are the main challenges ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
