\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Behaghel1909}
\citation{Futrell2015,Liu2008,Temperley2007}
\citation{Futrell2015}
\citation{Hopfield1982}
\citation{Gibson1998}
\citation{Grodner2005,Vasishth2011}
\citation{Hahn2021}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Architecture and the tokenizer of Language Models}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Discussion}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Effects of tokenization methods on surprisal measures}{3}{subsection.3.1}\protected@file@percent }
\bibcite{Behaghel1909}{{1}{}{{}}{{}}}
\bibcite{Futrell2015}{{2}{}{{}}{{}}}
\bibcite{Liu2008}{{3}{}{{}}{{}}}
\bibcite{Temperley2007}{{4}{}{{}}{{}}}
\bibcite{Hopfield1982}{{5}{}{{}}{{}}}
\bibcite{Gibson1998}{{6}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The distribution of suprisal for two different tokenization methods: whitespace tokenizer (blue) and byte-pair encoding tokenizer (orange) on English corpus using NNLM and GPT-2 model. The BPE tokenizer results in a more skewed distribution with a longer tail, indicating that it produces higher surprisal values for certain tokens compared to the whitespace tokenizer. This suggests that the choice of tokenization method can impact the surprisal measures obtained from language models.}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:tokenization_effects}{{1}{4}{The distribution of suprisal for two different tokenization methods: whitespace tokenizer (blue) and byte-pair encoding tokenizer (orange) on English corpus using NNLM and GPT-2 model. The BPE tokenizer results in a more skewed distribution with a longer tail, indicating that it produces higher surprisal values for certain tokens compared to the whitespace tokenizer. This suggests that the choice of tokenization method can impact the surprisal measures obtained from language models}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}References}{4}{section.4}\protected@file@percent }
\bibcite{Grodner2005}{{7}{}{{}}{{}}}
\bibcite{Vasishth2011}{{8}{}{{}}{{}}}
\bibcite{Hahn2021}{{9}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{5}
