\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Behaghel1909}
\citation{Futrell2015,Liu2008,Temperley2007}
\citation{Futrell2015}
\citation{Hopfield1982}
\citation{Gibson1998}
\citation{Grodner2005,Vasishth2011}
\citation{Hahn2021}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Architecture and the tokenizer of Language Models}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Discussion}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Effects of tokenization methods on surprisal measures}{4}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The distribution of suprisal for two different tokenization methods: whitespace tokenizer (blue) and byte-pair encoding tokenizer (orange) on English corpus using NNLM and GPT-2 model. The BPE tokenizer results in a more skewed distribution with a longer tail, indicating that it produces higher surprisal values for certain tokens compared to the whitespace tokenizer. This suggests that the choice of tokenization method can impact the surprisal measures obtained from language models.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:tokenization_effects}{{1}{5}{The distribution of suprisal for two different tokenization methods: whitespace tokenizer (blue) and byte-pair encoding tokenizer (orange) on English corpus using NNLM and GPT-2 model. The BPE tokenizer results in a more skewed distribution with a longer tail, indicating that it produces higher surprisal values for certain tokens compared to the whitespace tokenizer. This suggests that the choice of tokenization method can impact the surprisal measures obtained from language models}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Observations on artificial corpora randomized}{5}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average increase: 341.4\%. Minimum increase: 198.0\% (en). Maximum increase: 576.6\% (jap). P-value range: 0.000 - 0.000 All p-values < 0.05}}{6}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average increase: 151.7\%. Minimum increase: 41.1\% (en) Maximum increase: 313.9\% (jap). P-value range: 0.000 - 0.000 All p-values < 0.05}}{6}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Statistical Summary: Dependency-Preserving Randomization relative to Complete Random. Dep-preserved increase compared to complete random: -44.1\%. Minimum increase: -52.0\% (en). Maximum increase: -38.7\% (jap). All p-values $<$ 0.05.}}{7}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Information Locality: Surprisal as a Function of Memory Size}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{NNLM Results:}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RNNLM Results:}{7}{section*.10}\protected@file@percent }
\newlabel{fig:exercise7_main}{{3.3}{8}{RNNLM Results:}{Item.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparing Across Languages:}{8}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Is the comparison straightforward?}{8}{section*.13}\protected@file@percent }
\bibcite{Behaghel1909}{{1}{}{{}}{{}}}
\bibcite{Futrell2015}{{2}{}{{}}{{}}}
\bibcite{Liu2008}{{3}{}{{}}{{}}}
\bibcite{Temperley2007}{{4}{}{{}}{{}}}
\bibcite{Hopfield1982}{{5}{}{{}}{{}}}
\bibcite{Gibson1998}{{6}{}{{}}{{}}}
\bibcite{Grodner2005}{{7}{}{{}}{{}}}
\bibcite{Vasishth2011}{{8}{}{{}}{{}}}
\bibcite{Hahn2021}{{9}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {section}{\numberline {4}References}{9}{section.4}\protected@file@percent }
\gdef \@abspage@last{9}
